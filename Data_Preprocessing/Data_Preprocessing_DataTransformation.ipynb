{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### **Data Preprocessing - Data Transformation**\n",
    "    ![Data_Transformation](./Images/7.png)\n",
    "    - #### **Normalization** Scaling data to a smaller range, usually [0, 1]. It's commonly used when features have different scales or units.\n",
    "    ![Normalization](./Images/8.png)\n",
    "    - ##### **Methods used to normalize data**\n",
    "    ![Methods_used_to_normalize_data](./Images/9.png)\n",
    "        - **Standardization** Adjusting data to have a mean of 0 and a standard deviation of 1, often used when data follows a Gaussian distribution.\n",
    "        - **Normalization vs. Standardization** Normalization is used for bounded data, while standardization is used when data varies widely in scale.\n",
    "        ![Normalization_vs._Standardization](./Images/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01160987, 0.0174148 , 0.02902467, 0.04063454, 0.09868389,\n",
       "        0.11609869, 0.46439475, 0.87074017]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "numpy_array = np.array([2,3,5,7,17,20,80,150])\n",
    "normalized_array = preprocessing.normalize([numpy_array])\n",
    "normalized_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22941573,  0.22941573],\n",
       "       [ 0.22941573,  0.22941573],\n",
       "       [ 1.14707867,  1.14707867],\n",
       "       [-1.60591014, -1.60591014]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0,0],[0,0],[1,1],[-2,-2]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## **Data Preprocessing - Data Transformation**\n",
    "    - #### **Attribute Selection**\n",
    "        - **Need for Attribute Subset Selection** Reduces the dimensionality of the data, removing irrelevant or redundant features, which can improve model performance.\n",
    "        ![Need_for_Attribute_Subset_Selection](./Images/11.png)\n",
    "        ![Attribute Subset Selection](./Images/12.png)\n",
    "    - #### **Discretization** Converting continuous data into discrete bins or categories, useful for algorithms that require categorical input.\n",
    "        ![Discretization](./Images/13.png)\n",
    "        ![K-mean](./Images/14.png)\n",
    "        ![K-mean](./Images/15.png)\n",
    "    - #### **Concept Hierarchy Generation** Creating a hierarchy of concepts by organizing attributes into different levels of abstraction.\n",
    "        ![Concept Hierarchy Generation](./Images/16.png)\n",
    "        ![City](./Images/17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
